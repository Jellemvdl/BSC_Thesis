{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization dataset & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset quac (/home/jelle-van-der-lee/.cache/huggingface/datasets/quac/plain_text/1.1.0/4170258e7e72d7c81bd6441b3f3489ea1544f0ff226ce61e22bb00c6e9d01fb6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec4d56eb27345d0b6d386306ac95009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jelle-van-der-lee/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jelle-van-der-lee/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jelle-van-der-lee/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jelle-van-der-lee/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jelle-van-der-lee/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quac = load_dataset(\"quac\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jelle-van-der-lee/.local/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "quac_new = quac.remove_columns(('background', 'followups', 'yesnos', 'orig_answers', 'wikipedia_page_title'))\n",
    "\n",
    "quac_train = pd.DataFrame.from_dict(quac_new[\"train\"])\n",
    "quac_val = pd.DataFrame.from_dict(quac_new[\"validation\"])\n",
    "\n",
    "def remove_CA(row):\n",
    "    answers = row['answers']\n",
    "    questions = row['questions']\n",
    "    turn_ids = row['turn_ids']\n",
    "    index = 0\n",
    "    index_to_remove = []\n",
    "    for answer in answers['texts']:\n",
    "        if (answer[0]=='CANNOTANSWER'):\n",
    "            index_to_remove.append(index)\n",
    "        index += 1\n",
    "    new_answers_texts = [j for i, j in enumerate(answers['texts']) if i not in index_to_remove]\n",
    "    new_answers_starts = [j for i, j in enumerate(answers['answer_starts']) if i not in index_to_remove]\n",
    "    new_answers = {'texts' : new_answers_texts, 'answer_starts' : new_answers_starts}\n",
    "    new_questions = [j for i, j in enumerate(questions) if i not in index_to_remove]\n",
    "    new_turn_ids = [j for i, j in enumerate(turn_ids) if i not in index_to_remove]\n",
    "    new_context = row['context'].rsplit(' ', 1)[0]\n",
    "    return row['dialogue_id'], row['section_title'], new_context, new_turn_ids, new_questions, new_answers\n",
    "     \n",
    "\n",
    "quac_train = quac_train.apply(lambda row: remove_CA(row), axis=1, result_type='broadcast')\n",
    "quac_val = quac_val.apply(lambda row: remove_CA(row), axis=1, result_type='broadcast')\n",
    "\n",
    "def define_prevQA(row):\n",
    "    prev_QA = []\n",
    "    prev_QA_2 = []\n",
    "    for i in range(len(row['questions'])):\n",
    "        if i == 0:\n",
    "            prev_QA = prev_QA + [ [ [ [] , [] ] ] ]\n",
    "        else:\n",
    "            prev_QA_2 = prev_QA_2 + [ [ [row['questions'][i-1]] , [row['answers']['texts'][i-1]] ] ]\n",
    "            prev_QA = prev_QA + [ prev_QA_2 ]\n",
    "\n",
    "    return prev_QA\n",
    "\n",
    "quac_train['prev_QA'] = quac_train.apply(lambda row: define_prevQA(row), axis=1)\n",
    "quac_val['prev_QA'] = quac_val.apply(lambda row: define_prevQA(row), axis=1)\n",
    "\n",
    "def explode(df):\n",
    "    df['tmp']=df.apply(lambda row: list(zip(row['questions'],row['turn_ids'],row['texts'],row['answer_starts'],row['prev_QA'])), axis=1) \n",
    "    df=df.explode('tmp')\n",
    "    df[['questions','turn_ids', 'texts', 'answer_starts', 'prev_QA']]=pd.DataFrame(df['tmp'].tolist(), index=df.index)\n",
    "    df.drop(columns='tmp', inplace=True)\n",
    "    return df\n",
    "\n",
    "quac_train = quac_train.join(pd.DataFrame(quac_train.pop('answers').values.tolist()))\n",
    "quac_val = quac_val.join(pd.DataFrame(quac_val.pop('answers').values.tolist()))\n",
    "\n",
    "quac_train = explode(quac_train)\n",
    "quac_val = explode(quac_val)\n",
    "\n",
    "quac_train = quac_train.reset_index(drop=True)\n",
    "quac_val = quac_val.reset_index(drop=True)\n",
    "\n",
    "quac_train.rename(columns={'questions': 'question', 'texts': 'text', 'answer_starts': 'answer_start'}, inplace=True)\n",
    "quac_val.rename(columns={'questions': 'question', 'texts': 'text', 'answer_starts': 'answer_start'}, inplace=True)\n",
    "\n",
    "answer_columns = ['text', 'answer_start']\n",
    "\n",
    "quac_train['answers'] = quac_train[answer_columns].to_dict(orient='records')\n",
    "quac_val['answers'] = quac_val[answer_columns].to_dict(orient='records')\n",
    "\n",
    "quac_train = quac_train.drop(columns=answer_columns)\n",
    "quac_val = quac_val.drop(columns=answer_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "quac_val.drop(quac_val[quac_val['section_title']== 'Entry into politics'].index, inplace = True)\n",
    "quac_val.reset_index()\n",
    "def turn_id(row):\n",
    "    if ((row['turn_ids'][-1:])=='0'):\n",
    "        return 0\n",
    "    else:\n",
    "        return int(len(row['prev_QA']))\n",
    "    \n",
    "quac_train['turn_ids'] = quac_train.apply(lambda row: turn_id(row), axis=1)\n",
    "quac_val['turn_ids'] = quac_val.apply(lambda row: turn_id(row), axis=1)\n",
    "\n",
    "quac_train['turn_ids'] = quac_train['turn_ids'] + 1\n",
    "quac_val['turn_ids'] = quac_val['turn_ids'] + 1\n",
    "quac_train.rename(columns={'turn_ids': 'question_no'}, inplace=True)\n",
    "quac_val.rename(columns={'turn_ids': 'question_no'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "max_length = 384\n",
    "doc_stride = 128\n",
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i] \n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_history(row):\n",
    "    question_no = row[\"question_no\"]\n",
    "    prev_QA = row[\"prev_QA\"]\n",
    "    if (question_no == 1):\n",
    "        return (row[\"question\"])\n",
    "    try: \n",
    "        prev_ans = prev_QA[question_no - 2][1][0][0]\n",
    "    except: \n",
    "        return (row[\"question\"])\n",
    "    prev_question = prev_QA[question_no - 2][0][0]\n",
    "    return (prev_question + \" \" + prev_ans + \" \" + row[\"question\"])\n",
    "\n",
    "quac_train['question'] = quac_train.apply(lambda row: add_history(row), axis=1)\n",
    "quac_val['question'] = quac_val.apply(lambda row: add_history(row), axis=1)\n",
    "\n",
    "\n",
    "# def add_history(row):\n",
    "#     question_no = row[\"question_no\"]\n",
    "#     prev_QA = row[\"prev_QA\"]\n",
    "#     if (question_no == 1):\n",
    "#         return (row[\"question\"])\n",
    "#     elif (question_no == 2):\n",
    "#         try: \n",
    "#             prev_ans = prev_QA[question_no - 2][1][0][0]\n",
    "#         except: \n",
    "#             return (row[\"question\"])\n",
    "#         prev_question = prev_QA[question_no - 2][0][0]\n",
    "#         return (prev_question + \" \" + prev_ans + \" \" + row[\"question\"])\n",
    "#     else:\n",
    "#         try: \n",
    "#             prev_ans_1 = prev_QA[question_no - 2][1][0][0]\n",
    "#             prev_ans_2 = prev_QA[question_no - 3][1][0][0]\n",
    "#         except: \n",
    "#             return (row[\"question\"])\n",
    "#         prev_question_1 = prev_QA[question_no - 2][0][0] \n",
    "#         prev_question_2 = prev_QA[question_no - 3][0][0] \n",
    "#     #     print(prev_ans)\n",
    "#         return (prev_question_2 + \" \" + prev_ans_2 + \" \" + prev_question_1 + \" \" + prev_ans_1 + \" \" + row[\"question\"])\n",
    "\n",
    "# quac_train['question'] = quac_train.apply(lambda row: add_history(row), axis=1)\n",
    "# quac_val['question'] = quac_val.apply(lambda row: add_history(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "quac_train_dataset = quac_train.drop(['question_no', 'prev_QA'], axis=1)\n",
    "quac_val_dataset = quac_val.drop(['question_no', 'prev_QA'], axis=1)\n",
    "\n",
    "quac_train = quac_train.drop(['question_no', 'prev_QA'], axis=1)\n",
    "quac_val = quac_val.drop(['question_no', 'prev_QA'], axis=1)\n",
    "\n",
    "quac_train.rename(columns={'dialogue_id': 'id', 'section_title': 'title'}, inplace=True)\n",
    "quac_val.rename(columns={'dialogue_id': 'id', 'section_title': 'title'}, inplace=True)\n",
    "\n",
    "quac_train = quac_train.reset_index(drop=True)\n",
    "quac_val = quac_val.reset_index(drop=True)\n",
    "\n",
    "quac_train_dataset = Dataset.from_pandas(quac_train)\n",
    "quac_val_dataset = Dataset.from_pandas(quac_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 69109\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 5868\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "\n",
    "quac_new = DatasetDict({'train': quac_train_dataset, 'validation': quac_val_dataset})\n",
    "quac_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize & train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a4ed5b45194096aee01bba28021252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573c03d5c23d4e48acae624c97652ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_quac = quac_new.map(preprocess_function_squad, batched=True)\n",
    "# tokenized_quac = quac_new.map(preprocess_function, batched=True, remove_columns=quac_new[\"train\"].column_names)\n",
    "tokenized_quac = quac_new.map(prepare_train_features, batched=True, remove_columns=quac_new[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jelle-van-der-lee/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jelle-van-der-lee/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DefaultDataCollator, EarlyStoppingCallback, IntervalStrategy\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model_base = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=bsc_AI_thesis\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "%env WANDB_PROJECT=bsc_AI_thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "model_base.half()\n",
    "for layer in model_base.modules():\n",
    "    if isinstance(layer, torch.nn.BatchNorm2d):\n",
    "        layer.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rouge_score\n",
    "# !pip install evaluate\n",
    "\n",
    "# import evaluate\n",
    "# import numpy as np\n",
    "# def compute_metrics(eval_preds):\n",
    "#     metric = evaluate.load(\"rouge\")\n",
    "#     logits, labels = eval_preds\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "/home/jelle-van-der-lee/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 156477\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 391200\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jelle-van-der-lee/Desktop/Thesis/wandb/run-20221223_104131-b5eulsis</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jellemvdl/bsc_AI_thesis/runs/b5eulsis\" target=\"_blank\">bert-base-uncased-finetuned-quac-1QA</a></strong> to <a href=\"https://wandb.ai/jellemvdl/bsc_AI_thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78240' max='391200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 78240/391200 2:50:29 < 11:21:58, 7.65 it/s, Epoch 6/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 15044\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-quac-1QA/checkpoint-13040\n",
      "Configuration saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-13040/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-13040/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-13040/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-13040/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15044\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-quac-1QA/checkpoint-26080\n",
      "Configuration saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-26080/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-26080/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-26080/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-26080/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15044\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-quac-1QA/checkpoint-39120\n",
      "Configuration saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-39120/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-39120/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-39120/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-39120/special_tokens_map.json\n",
      "Deleting older checkpoint [bert-base-uncased-finetuned-quac-1QA/checkpoint-26080] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15044\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-quac-1QA/checkpoint-52160\n",
      "Configuration saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-52160/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-52160/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-52160/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-52160/special_tokens_map.json\n",
      "Deleting older checkpoint [bert-base-uncased-finetuned-quac-1QA/checkpoint-39120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15044\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-quac-1QA/checkpoint-65200\n",
      "Configuration saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-65200/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-65200/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-65200/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-65200/special_tokens_map.json\n",
      "Deleting older checkpoint [bert-base-uncased-finetuned-quac-1QA/checkpoint-52160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15044\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-quac-1QA/checkpoint-78240\n",
      "Configuration saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-78240/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-78240/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-78240/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-quac-1QA/checkpoint-78240/special_tokens_map.json\n",
      "Deleting older checkpoint [bert-base-uncased-finetuned-quac-1QA/checkpoint-65200] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-base-uncased-finetuned-quac-1QA/checkpoint-13040 (score: nan).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=78240, training_loss=0.01944774791506902, metrics={'train_runtime': 10229.9357, 'train_samples_per_second': 458.88, 'train_steps_per_second': 38.241, 'total_flos': 1.8399119671700582e+17, 'train_loss': 0.01944774791506902, 'epoch': 6.0})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    \"bert-base-uncased-finetuned-quac-1QA\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit = 2,\n",
    "    load_best_model_at_end=True,\n",
    "#     metric_for_best_model = 'f1',\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"bert-base-uncased-finetuned-quac-1QA\",\n",
    ")\n",
    "\n",
    "trainer_quac = Trainer(\n",
    "    model=model_base,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_quac[\"train\"],\n",
    "    eval_dataset=tokenized_quac[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)],\n",
    "    data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer_quac.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aca2e1e2f2f4a99aaf9ea29b7edfc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/runtime</td><td>▄▂▂▁█▂</td></tr><tr><td>eval/samples_per_second</td><td>▅▇▇█▁▇</td></tr><tr><td>eval/steps_per_second</td><td>▅▇▇█▁▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>nan</td></tr><tr><td>eval/runtime</td><td>46.3119</td></tr><tr><td>eval/samples_per_second</td><td>324.841</td></tr><tr><td>eval/steps_per_second</td><td>27.077</td></tr><tr><td>train/epoch</td><td>6.0</td></tr><tr><td>train/global_step</td><td>78240</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.0</td></tr><tr><td>train/total_flos</td><td>1.8399119671700582e+17</td></tr><tr><td>train/train_loss</td><td>0.01945</td></tr><tr><td>train/train_runtime</td><td>10229.9357</td></tr><tr><td>train/train_samples_per_second</td><td>458.88</td></tr><tr><td>train/train_steps_per_second</td><td>38.241</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">bert-base-uncased-finetuned-quac-1QA</strong>: <a href=\"https://wandb.ai/jellemvdl/bsc_AI_thesis/runs/b5eulsis\" target=\"_blank\">https://wandb.ai/jellemvdl/bsc_AI_thesis/runs/b5eulsis</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221223_104131-b5eulsis/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
